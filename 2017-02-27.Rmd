---
title: "STA221"
author: "Neil Montgomery"
date: "Last edited: `r format(Sys.time(), '%Y-%m-%d %H:%M')`"
output: 
  beamer_presentation:
    keep_tex: TRUE
    incremental: TRUE
    df_print: tibble
    fig_caption: FALSE
classoption: aspectratio=169
header-includes:
- \newcommand{\ve}{\varepsilon}
- \newcommand{\dbar}[1]{\overline{\overline{#1}}}
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE,
                      dev='pdf', fig.width=5, fig.asp=0.618, fig.align='center')
options(tibble.width=70, scipen = 999, tibble.print_min=5, show.signif.stars = FALSE)
library(tidyverse)
library(readxl)
```


```{r}
bodyfat <- read_csv("Body_fat.csv")

options(scipen=6)
bf_wt <- bodyfat %>% 
  lm(`Pct BF` ~ Weight, data = .) 

bf_ht <- bodyfat %>% 
  lm(`Pct BF` ~ Height, data = .) 
```

## the sample correlation coefficient

Recall this expression that is used in the formula for $b_1$:
$$S_{xy}=\sum_{i=1}^n \left(x_i-\overline x\right)
\left(y_i - \overline y\right)$$
\pause This is a (crude) measure of the linear association between dataset variables with names $x$ and $y$. 

\pause It turns out to be a variation on something called a "sample covariance":
$$s_{xy} = \frac{\sum_{i=1}^n \left(x_i-\overline x\right)
\left(y_i - \overline y\right)}{n-1}$$
(I'm using the textbook's Chapter 6 notation which is inadvertently close to my own $S_{xy}$ notation. Sorry!)

## the sample correlation coefficient

The sample covariance $s_{xy}$ depends on the unit of measurement for both variables, when all we care about is the strength of the relationship.

We can divide out the variation in both $x$ and $y$ to obtain what is called the \textit{sample correlation coefficient:}

\pause
$$r = \frac{s_{xy}}{s_xs_y} = \frac{S_{xy}}{\sqrt{S_{xx}S_{yy}}}$$
where $s_x$ and $s_y$ are the sample standard deviations of the $x$ and $y$ variables, and the final expression because the $n-1$ cancels top and bottom. 

\pause \textit{The sample mean estimates the mean... The sample variance estimates the variance... The sample correlation coefficient does in fact estimate a true, unknown "correlation coefficient", which is called $\rho$, but whose details we will not investigate, other than to point out that it is a number that assesses the strenght of the linear relationship between two distributions.}

## properties of the sample correlation coefficient

It is symmetric in $x$ and $y$. There is not (necessarily) an "input" and an "output" variable. 

\pause 

$$(r)^2 = R^2 = \frac{SSR}{SST} = 1 - \frac{SSE}{SST}$$ 

\pause 
$$-1 \le r \le 1$$

\pause The sample correlation coefficient is only suitable when the relationship is linear, and is susceptible to all the same shortcomings as any regression model.

\pause CORRECTED!
$$r = b_1\sqrt{\frac{S_{yy}}{S_{xx}}}$$
where $b_1$ is the slope estimator with $x$ is "input"...

## examples



```{r, message=FALSE}
library(dplyr)
library(ggplot2)
set.seed(2)
n <- 40
x <- seq(1, 10, length.out = n)
y1 <-  1 + x + rnorm(n, 0, 0.1)

y2 <- 1 - x + rnorm(n, 0, 1)

y3 <- 1 + x + rnorm(n, 0, 5)

set.seed(5)
y4 <- 1 + x + rnorm(n, 0, 2)

y5 <- 20 - (x - 8)^2 + rnorm(n, 0, 8)


set.seed(42)
y6 <- 1 + x + rnorm(n, 0, 2)

x2 <- seq(1, 10, length.out = 2*n)
y7 <- 1 + x2 + rnorm(2*n, 0, 2)

p1 <- data.frame(x, y1) %>% 
  ggplot(aes(x=x, y=y1)) + geom_point() + 
  ggtitle(substitute(paste(r, " = ", r1), list(r1 = round(cor(x, y1), 3))))
p2 <- data.frame(x, y2) %>% 
  ggplot(aes(x=x, y=y2)) + geom_point() + 
  ggtitle(substitute(paste(r, " = ", r2), list(r2 = round(cor(x, y2), 3))))
p3 <- data.frame(x, y3) %>% 
  ggplot(aes(x=x, y=y3)) + geom_point() + 
  ggtitle(substitute(paste(r, " = ", r3), list(r3 = round(cor(x, y3), 3))))

y9 <- 10 + rnorm(n, 0, 3)
p4 <- data.frame(x, y9) %>% 
  ggplot(aes(x=x, y=y9)) + geom_point() + 
  ggtitle(substitute(paste(r, " = ", r9), list(r9 = round(cor(x, y9), 3))))


y10 <- 10 + (x - mean(x))^2 + rnorm(n, 0, 0.01)
p5 <- data.frame(x, y10) %>% 
  ggplot(aes(x=x, y=y10)) + geom_point() + 
  ggtitle(substitute(paste(r, " = ", r10), list(r10 = round(cor(x, y10), 3))))



x3 <- c(x, max(x) + 12.5)
y11 <- c(rnorm(n, 0, 0.25), 3)
p6 <- data.frame(x3, y11) %>% 
  ggplot(aes(x=x3, y=y11)) + geom_point() + 
  ggtitle(substitute(paste(r, " = ", r11), list(r11 = round(cor(x3, y11), 3))))

source("multiplot.R")
multiplot(p1,p2,p3,p4,p5,p6,cols=3)
```

## inference for correlation coefficient

Since $b_1$ has a normal distribtion, it might not come as a surprise the $r$ also has a normal distribution. In fact:
$$\frac{r\sqrt{n-2}}{\sqrt{1-r^2}} \sim t_{n-2}$$

\pause So it is possible to do hypothesis testing for $H_0: \rho=0$ versus $H_a: \rho\ne 0$.

\pause (Note: confidence interval is also possible, but this is best left to the computer.)

## bodyfat example

Recall the dataset:

```{r}
bodyfat
```

I wonder if the correlation between `Neck` and `Chest` circumferences is non-zero.

## example - correlation matrix

A very useful information display is a "correlation matrix". Focus on the nine displayed variables, excluding `Age`:

```{r}
options(digits = 3)
select(bodyfat, c(1, 3:9)) %>% cor(.)
```

## `Neck` versus `Chest`

```{r}
bodyfat %>% 
  ggplot(aes(x=Neck, y=Chest)) + geom_point()
```

## correlation analysis

```{r}
with(bodyfat, cor.test(Neck, Chest))
```

## another example: `Pct BF` versus `Height`

Recall:

```{r}
short_print_lm(summary(bf_ht))
```

## compare p-value of 0.644 for $H_0: \beta_1 = 0$

Now the correlation analysis:

```{r}
options(digits=6)
with(bodyfat, cor.test(`Pct BF`, `Height`))
```
Not a coincidence! The conclusion must be identical.

# the analysis of designed experiments

## Formal definitions: factor, level 

A *factor* is a controllable experimental condition.

\pause A factor can take on two or more *levels*. 

\pause E.g., in a study of haul trucks "oil brand" could be a factor, with levels "Castrol", "Volvo", "Komatsu". 

\pause When experimental units are randomly assigned to levels of a factor and some output measure is observed, this is called a *designed experiment*. The formal model is typically written as (more on this later):
$$ Y_{ij} = \mu_i + \ve_{ij}$$

\pause You've seen the case of $i \in \{1, 2\}$---such an experiment would be analyzed using a two-sample $t$ procedure.

\pause In reality any dataset with one categorical "input" variable and one numerical "output" variable will be analysed the same as a formally designed experiment.

## Typical dataset...

```{r}
library(knitr)
set.seed(1)
oil <- data.frame("Truck ID" = replicate(12, paste("HT", sample(0:999, 1, replace = TRUE))), 
                  "Oil" = rep(c("Volvo", "Castrol", "Komatsu"), 4),
                  "Viscosity" = round(rnorm(12, 25, 1), 1))
kable(oil)
```

## One factor notation, models

"Balanced" case with equal sample size $n$ for each of $k$ levels for $N = nk$ total.

Levels:    |  1  |  2  | ... |  i  | ... |  k  |
:---------|:---:|:---:|:---:|:---:|:---:|:---:|
&nbsp;    | $y_{11}$ | $y_{21}$ | ... | $y_{i1}$ | ... | $y_{k1}$ |
&nbsp;    | $y_{12}$ | $y_{22}$ | ... | $y_{i2}$ | ... | $y_{k2}$ |
&nbsp;    | $\vdots$ | $\vdots$ | &nbsp; | $\vdots$ |&nbsp; | $\vdots$  |
&nbsp;    | $y_{1n}$ | $y_{2n}$ | ... | $y_{in}$ | ... | $y_{kn}$ |
Sample average: | $\overline y_{1}$ | $\overline y_{2}$ | ... | $\overline y_{i}$ | ... | $\overline y_{k}$ |

Grand overall average: $\dbar{y}$

Models:
$$y_{ij} = \mu_i + \ve_{ij}, \qquad \ve_{ij} \text{ i.i.d. } N(0, \sigma^2)$$
$$y_{ij}= \mu + \alpha_i + \ve_{ij}, \qquad \sum \alpha_i = 0 \qquad \ve_{ij} \text{ i.i.d. } N(0, \sigma^2)$$

## The main question 

The main question is $H_0: \mu_1 = \mu_2 = \cdots = \mu_k$ versus the negation (equivalently: all the $\alpha_i = 0$.) 

In other words "is the variation among all the $y_{ij}$ due to the factor variable, or just due to random chance?". The analysis even follows this logic. 

The variation among the $y_{ij}$ is quantified as (as usual?):

$$(N-1)\cdot s^2_y = \sum_{i=1}^k\sum_{j=1}^n \left(y_{ij} - \dbar{y}\right)^2$$

We will split this up into the "factor" part and the "random chance" part (like done in regression).
